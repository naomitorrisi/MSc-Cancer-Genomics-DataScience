---
title: "R Programming Coursework"
author: "Naomi Danielle Torrisi"
output: html_notebook
---
### Assessment 1 30%

# Question 1: Load the file Sales.tsv with the first column as the rownames (2%)

### Setting up

To prepare for this coursework, I downloaded the assessment.zip file, which automatically created a folder named Assessment containing all the required input files: "Counts.tsv", "Genes.tsv", "Sales.tsv", "SampleData.tsv". 
This folder is located in my downloads directory: /Users/naomi/Downloads/R Programming Coursework - Naomi Torrisi/
An additional folder named Output is currently empty and will store results generated by the analysis, such as tables and plots stored in the same directory as Assessment.


### Installing packages

In this section, I installed the tidyverse and ggpubr packages, which extend R's functionality for data manipulation and visualisation. In R, many useful functions are not built into the base software but instead come from external packages developed by the R community. I installed these packages to enhance the analytical and plotting capabilities of my R environment for this coursework. 

The tidyverse (short for 'tidy data universe') is a collection of packages such as dplyr, tibble, readr, and ggplot2 designed to make data manipulation, cleaning, and visualisation consistent, readable and efficient. 
The ggpubr package (short for 'ggplot2 + pubr' meaning 'grammar of graphics, publication ready') builds upon ggplot2 by providing functions for adding statistical results (eg. correlations, p-values, regression lines), customising themes and labels, and combining multiple plots into a single professional figure.


```{r}
## Install the tidyverse and ggpubr packages to extend base R functionality for data manipulation and visualisation (only needs to be done once per setup)
## tidyverse adds functions for clean, consistent data analysis; ggpubr adds functions for creating publication-ready plots.
install.packages("tidyverse")
install.packages("ggpubr")
install.packages("emmeans")
install.packages("multcompView")
```


### Loading files

In this section, I imported the Sales.tsv dataset into R using read.delim() function. Because .tsv files are tab-separated by default, the read.delim() function automatically recognises tab spacing between columns, so there was no need to manually specify a separator argument. Please run this chunk of code:


```{r}
## Load in the Sales dataset
Sales <- read.delim('/Users/naomi/Downloads/R Programming Coursework - Naomi Torrisi /Assessment/Sales.tsv')
Sales
## Note: read.delim() automatically recognises tab-separated values (.tsv) files
## So there is no need to manually specify the separator argument (sep = "\t").
## This function reads the file into a dataframe where each column represents a variable.
```

Once loaded, the resulting dataframe, named Sales, displayed the monthly sales values for each year, which can now  be used for subsequent data manipulation and analyses. 

### Column and Rownames

By default, when using the read.delim() function in R, every column in a .tsv file is treated as a regular variable. In the sales.tsv dataset, however, the first column represents identifiers (years) rather than variables to be analysed. Therefore, it's more appropriate to use this column as the row names rather than as a data column. 

To manage row names effectively, I used functions from the tibble package which is part of the tidyverse suite. This allowed me to move the "Year" column into the row name position using column_to_rownames(), ensuring that each row is now uniquely identified by its corresponding year. 

This step is useful for subsequent analyses, such as calculating yearly averages or plotting trends over time, because this preserves numeric columns for analysis but removes the Year column from future summaries. Thus, it allows R to recognise each year as a distinct observation rather than as part of the data to be summarised. 

```{r}
## Load tibble package (from tidyverse) to manage rownames
library(tibble)
## Convert the 'Year' column to rownames using the tibble package
## This moves the Year values into the row labels and removes the column from the dataframe, allowing each row to be uniquely identified by year for later analysis
Sales <- column_to_rownames(Sales, "Year")
## View dataframe to confirm that 'Year' is now used as rownames
Sales
```

This instructs R to use the 'Year' column name as the row labels instead. I did this to make the data easier to understand and work with as the IDs are now clearly assigned to each row. 


# Question 2:  Part 1 - Calculate the mean of each row (3%). 

In this section, I calculated the average sales for each year by taking the mean across all twelve monthly columns using the rowMeans() function. This produced a single summary value for each year, representing the overall average sales across all months. 
These mean values were displayed directly rather than added to the main dataset, as it is good practice to keep derived statistics separate so that they can be easily recalculated if the underlying data changes.

```{r}
## Calculate the average sales for each year using rowMeans()
print("Average sales for each year:")
row_means <- rowMeans(Sales)
row_means
```
### Results Interpretation

The yearly averages show that sales have increased steadily over time, with the highest mean sales recorded in 2022 and the lowest in 2008. This upward trend suggests a long-term improvement in performance, with more recent years showing much stronger overall sales compared to the earlier part of the dataset.

# Question 2:  Part 2 - List the top 5 years with the highest mean sales.

I sorted the calculated yearly averages in descending order with sort(decreasing = TRUE) to identify which years had the highest overall sales. I then subsetted the sorted list by indices [1:5] to extract the top five years, providing a clear summary of the dataset's five best-performing years in terms of mean sales.

```{r}
## Sort rows in descending order of mean sales for each year
## rowMeans() calculates the average across all 12 monthly columns
## Sort (decreasing = TRUE) orders the results from highest to lowest
print("Highest to Lowest Average Yearly Sales:")
sorted_means <- Sales %>% rowMeans() %>% sort(decreasing = TRUE)
sorted_means

## List top 5 years with the highest mean sales
## names() extracts only the year labels, and [1:5] subsets the first five indices
print("Top Five Best Performing Years:")
Top5Years <- names(sorted_means[1:5])
Top5Years

## Note: Averages are kept separate from the dataset to preserve the integrity of the raw data.
```
### Results Interpretation

The five years with the highest average sales were 2022, 2021, 2020, 2019, and 2017. This shows that sales performance has steadily improved in recent years, with 2022 reaching the highest overall average. The upward trend suggests consistent growth and stronger market performance over time.

# Question 3: Part 1 - Calculate the median of each column (5%).

In this section, I calculated the median sales value for each month across all years using the apply() function. The argument 2 tells R to apply the function across columns rather than rows, and median() returns the middle value for each monthly column. 
I did not store the median values directly inside the main dataset, as it's best practice to keep summary statistics separate. This ensures that if the data is later updated or corrected, all summary values can be easily recalculculated without altering the original dataset.

```{r}
## Calculate median sales for each month (across all years)
## Use the argument (2) which tells R to apply the function to columns only
## median() returns the middle value of each column
print("Median sales for each month across all years:")
col_medians <- apply(Sales, 2, median)
col_medians

## Note: Do not store summary statistics directly in the data table
## Keeping calculated values separate allows easy recalculation if the data changes
```

### Results Interpretation

Monthly sales medians ranged from around 515 in April to about 707 in March. March had the strongest sales overall, while April showed the weakest performance. This pattern suggests that sales tend to be higher at the start of the year and drop slightly in spring before levelling out through the rest of the year.

# Question 3: Part 2 - List the top 5 months with the highest median sales.

In this section, I identified the five months with the highest median sales values. I first sorted the column medians (calculated using apply(Sales, 2, median)) in descending order using sort() function with the argument decreasing = TRUE. This arranged monthly medians from highest to lowest. I then used subsetting [1:5] along with names() function to extract the names of the top five months, providing a quick summary of which months had the strongest sale performance.

```{r}
## Sort the monthly median sales values in descending order (highest to lowest)
sorted_medians <- sort(col_medians, decreasing = TRUE)
sorted_medians

## Select the top 5 months 
## names() extracts the month names
## [1:5] subsets the first five indices
Top5Months <- names(sorted_medians[1:5])
Top5Months
```

### Results Interpretation

The five months with the highest median sales were March, June, September, January, and May. This shows that sales tended to peak during early spring and mid-year, with another smaller rise after the new year. These patterns suggest possible seasonal trends, where certain times of year consistently perform better than others.

# Question 4: Part 1 - Generate a scatter plot of the sales for the 2 months with the highest median values (5%). Add a linear regression line and correlation statistics to this plot.


The scatterplot was generated using ggplot2 for visualisation and ggpubr for adding statistical annotations. 

These packages allowed the inclusion of linear regression line to visualise the strength of relationship between the two months with the highest median sales, as well as the automatic calculation and display of the Pearson correlation coefficient (r) and p-value to quantify the strength and significance of the relationship between both variables.

The p-value was less than 0.05, indicating that the observed relationship is statistically significant.

```{r}
## Select top 2 months from highest median sales values
## names() extracts the column names and [1:2] subsets the first two (highest) entries
Top2Months <- names(sorted_medians[1:2])
Top2Months

## Load required packages for plotting
library(ggplot2) ## Used to create plots
library(ggpubr) ## Provides functions for adding correlation, regression and annotations

## Create a scatter plot comparing sales between the two highest-median months
ggplot(Sales, aes(x = .data[[Top2Months[1]]], y = .data[[Top2Months[2]]])) +
  
  ## Plot orange points to show individual yearly sales values
         geom_point(colour = "orange", size = 2) + 
  
  ## Add a purple linear regression line to show the trend between the two months
         geom_smooth(method = "lm", se = TRUE, colour = "purple") +
  
  ## Display Pearson correlation coefficient (r) and p-value on the plot
         stat_cor(method = "pearson", colour = "black", label.x.npc = "left", label.y.npc = "top", size = 4) + 
  
  ## Display the regression equation and R^2 value as text
         stat_regline_equation(
  aes(label = paste(..eq.label.., ..rr.label.., sep = "   ")), ## Include equation and R^2
  label.x.npc = "left", label.y.npc = "top", vjust = 5,
  size = 4, output.type = "text"  ## Shows the equation and R^2 value as normal text instead of turning them into a math formula
) +
  
  ## Add the axis labels and plot title automatically using the month names
  labs(
    x = Top2Months[1],
    y = Top2Months[2],
   title = paste("Sales Comparison:", Top2Months[1], "vs", Top2Months[2])
) +
  ## Apply a minimalistic theme for better readability
         theme_minimal()
```

### Results Interpretation

The scatter plot shows a clear positive trend between sales in March and June; when sales were high in March, they were also high in June. The linear regression line confirms this strong relationship (R = 0.90, p < 0.001), meaning the two months follow a very similar sales pattern. This suggests that the same seasonal or market factors likely influenced performance in both months.

# Question 5: Convert the rownames into a column and turn the data into long format so that the data can be processed using ggplot2 (5%)

In this section, I changed the Sales data from a wide to a long format so that it could be used with ggplot2. In the original dataset, each year was a single row and the sales for each month were spread across separate columns. To fix this, I used rownames_to_column() from the tibble package to turn the row names (years) into a proper column called "Year". I then used pivot_longer() from tidyr to gather all the monthly columns into two columns; "Month" for the month names and "Sales" for their values. This created one row per year-month combination, which is the structure that ggplot2 needs for plotting grouped data. 


```{r}
## Load required packages (tibble and tidyr are part of tidyverse) to use rownames_to_column() and pivot_longer() functions
library(tibble)
library(tidyr)

## Convert the Sales data from wide format to long format for use with ggplot2
Sales_long <- Sales %>% 
        rownames_to_column("Year") %>% ## Move rownames into a new column called 'Year'
        pivot_longer(cols = -Year, ## Transform all other columns (months) except 'Year'
                     names_to = "Month", ## Create a new column 'Month' containing former column names
                     values_to = "Sales") ## Create a new column 'Sales' containing the corresponding values
## View the resulting long-format dataframe
Sales_long
``` 
 
# Question 6: Part 1 - Using the long form dataframe, generate a Boxplot of the sales values for the 2 years with the highest mean sales (10%)
# Part 2: Determine whether to use a parametric or non-parametric test for these data.
# Part 3: Include the correct statistics on the plot.

### Identifying and Filtering Years
 
In this section, I first loaded the required packages for data manipulation, excluding those that were already loaded in earlier chunks (such as ggplot2 and ggpubr) as these only need to be loaded once per run. In this case, I loaded the dplyr package to use its key functions like filter(), group_by(), and summarise(). 

I then identified the two years with the highest mean sales from the object sorted_means, using which I had previously created by calculating the average sales for each year and sorting the results in descending order. This allowed me to isolate the best-performing years for further analysis. Using [1:2] slices the first two elements from the vector sorted_means and extracts the named numeric vector (the corresponding years "2021" and "2022") for further filtering and plotting.

Next, I used the filter() function from the dplyr package to extract only those two years from the long-format dataset Sales_long. I used the pipe operator (%>%) which passes the result of one function directly into the next and the "is in" operator (%in%) which tests whether each year is included in the Top2Years vector. Combined, these make the filtering process concise and readable, improving the overall efficiency of the data manipulation workflow. Only those rows were retained, leaving monthly sales data for the two top-performing years.

Finally, I converted the Year column into a factor variable with levels matching Top2Years, ensuring that when visualised in ggplot2, the years appear on the x-axis in the correct ranked order (rather than alphabetically). This preserves the logical structure of the anaysis and makes the final boxplot easier to interpret:

```{r}
## Load packages at the start
library(dplyr)  ## for data manipulation (filter, group_by, summarise)

## NOTE: I do not need to reload ggplot2 or ggppub2 as these are previously loaded in question 4. Make sure to run question 4 code block before running this code block.

## Identify the top two years from the already-sorted means (long format already created in Question 5)
print("Two Best-Performing Sales Years:")
Top2Years <- names(sorted_means[1:2])
Top2Years

## Filter the long-format data for these top two years
Sales_top2 <- Sales_long %>% 
  filter(Year %in% Top2Years)

## Ensure Year is a factor so it appears neatly on x-axis
Sales_top2$Year <- factor(Sales_top2$Year, levels = Top2Years)
```

### Assessing Normality and Parametric Decision

In this section, I assessed whether the monthly sales for the two top-performing years followed a normal distribution so I could select the appropriate statistical test. 
To do this, I used the group_by() and summarise() functions from the dplyr package to apply the Shapiro-Wilk test separately to each year's sales values.

I chose the Shapiro-Wilk test as it is reliable for small datasets (ie 12 monthly values per year). Both 2021 (p=0.74) and 2022 (p=0.13) returned p-values greater than 0.05, meaning neither distrubtion significantly deviated from normality:

```{r}
## Assess the normality of montly sales for each of the top 2 years using a Shapiro-Wilk test
Sales_top2 %>% 
  group_by(Year) %>%   ## group the data by Year so each year is tested separately
  summarise(p_value = shapiro.test(Sales)$p.value) ## perform the Shapiro–Wilk test on Sales within each group ## and extract the resulting p-value for normality assessment
## Both years have p > 0.05 (Shapiro–Wilk), confirming normality. Proceed with a parametric Welch's t-test.
```

Since the data met the assumption of normality, I used a parametric Welch’s two-sample t-test to compare the mean monthly sales between the two years. I chose the Welch test because, unlike the standard t-test, it doesn’t assume that both groups have equal variance. This makes it a safer and more reliable choice when the variability between the two years might not be exactly the same.

```{r}
## Conduct Welch's t-test
test_result <- t.test(Sales ~ Year, data = Sales_top2)
test_result
```

I then extracted key summary statistics from the test output including; t-value (t_val), degrees of freedom (df_val), p-value (p_val) and 95% confidence interval (ci_high and ci_low), rounded to two decimal places for readability:

```{r}
## Extract key test statistics for reporting
t_val <- round(test_result$statistic, 2) ## extract the t-value from the t-test output and round to 2 decimal places
df_val <- round(test_result$parameter, 1) ## extract the degrees of freedom (df) and round to 1 decimal place
p_val <- signif(test_result$p.value, 3) ## extract the p-value and round to 3 significant figures
ci_low <- round(test_result$conf.int[1], 2) ## extract the lower bound of the 95% confidence interval and round to 2 decimals
ci_high <- round(test_result$conf.int[2], 2) ## extract the upper bound of the 95% confidence interval and round to 2 decimals
```

These summary statistics where then combined into a single string (subtitle_txt) so that they could be displayed directly under the figure title, in a self-contained block that could be easily interpretable: 

```{r}
## Prepare a text string for subtitle with full stats
subtitle_txt <- paste0(      ## combine all extracted values into a single formatted text string
  "Welch's t-test: t(", df_val, ") = ", t_val,   ## display the test type, t-value, and degrees of freedom
  ", p = ", p_val,    ## include the p-value
  "; 95% CI [", ci_low, ", ", ci_high, "]"   ## include the 95% confidence interval limits
)
```

Finally, I defined y_max to find the highest sales value, which then allowed me to position the p-value label neatly just above the tallest box on the plot:

```{r}
## Define the maximum sales value to calculate where the p-value label should go
y_max <- max(Sales_top2$Sales, na.rm = TRUE)
```

### Plot the boxplot with the correct statistics included 

In this section, I created a boxplot to compare the distribution of monthly sales between the two top-performing years and to visually display the results of the statistical analysis. I used ggplot() function from the ggplot2 package to build the plot step-by-step, specifying that the x-axis represents the years, y axis represents the monthly sales values, and the boxes should be filled according to each year.

The geom_boxplot() function was then used to display the distribution of sales for each year, showing the median quartiles, and any potential outliers. I customised its appearance for clarity, setting width=0.6 to control the box width, alpha=0.6 to make the fill slightly transparent, and defining the outlier appearance with outlier.shape=1 and outlier.size=2. 

To make the plot more informative, I added geom_jitter() to show the individual monthly data points as small black dots. These points were slightly spread horizontally (width=0.1) so they didn't overlap, and made slightly transparent(alpha=0.7) for to see the overlapping points more clearly. 

I then used stat_compare_means() from the ggpubr package to automatically include the p-value from the Welch's t-test above the boxes. The function was set with method="t.test" to match the test I used earlier, and label="p.format" to display the p-value in an easy-to-read format such as "p=0.62". The label position was adjusted using label.x=1.5 to centre it between the boxes and label.y = y_max * 1.05 to place it slightly above the tallest boxplot, based on the maximum sales value. I also expanded the y-axis range slightly (expand_limits(y = y_max * 1.10)) to ensure the p-value label didn't overlap with the boxes.

Finally, I added informative axis labels and a title using labs() function with a subtitle that displays the key t-test statistics (t-value, degrees of freedom, p-value, and the 95% confidence interval) all combined earlier to give the variable subtitle_txt. 

The theme_minimal function was applied last to give the figure a clean, professional layout by removing uneccessary background lines and clutter.

```{r}
## Generate boxplots showing the sales distribution for each year
ggplot(Sales_top2, aes(x = Year, y = Sales, fill = Year)) + ## map Year to x-axis, Sales to y-axis and colour by Year
  geom_boxplot(  
    width = 0.6, ## adjust width of each box (default = 0.75)
    alpha = 0.6, ## adjust transparency of fill colour (1 is solid, 0 is transparent)
    outlier.shape = 1, ## define how outliers are drawn (16 = solid circle)
    outlier.size = 2) + ## make outliers slightly larger for visibility (default = 1.5)
## Note: No statistical outliers were detected within either year, 
## so no outlier points are visible on the boxplots.

## Overlay each month’s actual sales value as a jittered point
  geom_jitter(
    width = 0.1, ## shifts jitter horizontally by 0.1 units so points don't overlap
    alpha = 0.7, ## makes points slightly transparent for readability
    colour = "black" ## sets point colour to black to contrast with box colours
  ) + 

## Put the p-value at the midpoint between the two boxes (x = 1.5),
## and slightly above the tallest point. Also add headroom on the y-axis.
    stat_compare_means(
    method   = "t.test",   ## perform a Welch’s two-sample t-test between the two years
    label    = "p.format",  ## display the p-value in formatted form (e.g., "p = 0.042")
    label.x  = 1.5,         ## position the label horizontally at the midpoint between the two boxes     
    label.y  = y_max * 1.05   ## place the label slightly above the tallest boxplot
  ) + 
  expand_limits(y = y_max * 1.10) +  ## expand the y-axis range by 10% to ensure the p-value label is visible and not clipped
  
## Add axis labels and an informative title
  labs(title = "Monthly Sales Distribution for Top 2 Years", ## gives the plot an informative title 
       subtitle = subtitle_txt,  ## displays Welch’s t-test stats underneath the title
       x = "Year", ## labels the x-axis
       y = "Sales Value" ## labels the y-axis
  ) + 
  
## Apply a clean theme suitable for publication
  theme_minimal() ## will remove background gridlines and clutter to better focus on the data
```
### Results Interpretation

The boxplot shows that average monthly sales were slightly higher in 2022 compared with 2021, but the overlap in distributions is considerable. The Welch's t-test (p = 0.62 rounded to 2 decimal places) suggests that this difference is not statistically significant, meaning sales performance remained broadly consistent between the two years. 

### Assessment 2 30%


# Question 1: Use the three files (Counts.tsv, SampleInfo.tsv, Genes.tsv) provided to generate an object of the expressionSet class from the Biobase package (use the first column of these files as the rownames) (5%)

In this section, I created an ExpressionSet object using three input files: Counts.tsv, SampleInfo.tsv, and Genes.tsv. This type of object, from the Biobase package, is designed to store expression data and its associated metadata in one structured format. 
The expression matrix (Counts.tsv) contains gene expression counts with genes as rows and samples as columns. The SampleInfo.tsv file provides sample-level metadata (such as experimental group or condition), while Genes.tsv provides feature-level metadata describing each gene.
To ensure the data were linked correctly , I used the first column in each file as the rownames, matching genes and samples consistently across files. After importing the data, I verified that the sample names in SampleInfo matched the column names in Counts, and that the gene IDs in Genes matched the rownames in Counts. This alignment step is essential so that the ExpressionSet correctly associates expression values with the right samples and gene annotations. 
I first start by installing BiocManager package and loading the Biobase library for dealing with ExpressionSet objects as it good at organising biological experimental data.

### Installing and loading packages 

```{r}
## Install packages
install.packages("BiocManager")
```

```{r}
## Load Biobase package
library(Biobase)
```


### Import the three datafiles 

In this step, I loaded the three main data files (Counts.tsv, SampleData.tsv, and Genes.tsv) which together will form the building blocks of my ExpressionSet object. Each file holds a different type of information: Counts.tsv contains the gene expression values, SampleData.tsv describes each sample (for example, its condition or batch), and Genes.tsv provides details about the genes themselves. 

When importing the files, I told R to treat the first column as the row names, so each gene or sample could be uniquely identified:

```{r}
Counts <- read.delim("~/Downloads/R Programming Coursework - Naomi Torrisi /Assessment/Counts.tsv", row.names = 1)
SampleInfo <- read.delim("~/Downloads/R Programming Coursework - Naomi Torrisi /Assessment/SampleData.tsv", row.names = 1)
Genes <- read.delim("~/Downloads/R Programming Coursework - Naomi Torrisi /Assessment/Genes.tsv", row.names = 1)
```

### Check datasets align

Once the data were loaded, I manually checked that the column names in Counts matched row names in SampleInfo, and that the row names in Counts matches those in Genes. This is important because R does not automatically track which expression values belong to which gene or sample, making sure everything lines up properly and avoids later errors when combining the data into an ExpressionSet:

```{r}
## Check alignment between the three dataframes
identical(colnames(Counts), rownames(SampleInfo)) ## Samples should match between Counts and SampleInfo
identical(rownames(Counts), rownames(Genes)) ## Genes should match between Counts and Genes
```

If both statements return TRUE, it confirms that the data are correctly aligned and ready to be combined into a single ExpressionSet object.

If either check returns FALSE, it means one or more files are slightly out of order or contain mismatched IDs. In that case, the rows or columns can be reordered manually so everything lines up correctly before proceeding. In this case, both return TRUE so you can continue without manually aligning any of the datasets:

```{r}
## if any return FALSE please load this code
## to reorder SampleInfo and Genes to match Counts if needed:
SampleInfo <- SampleInfo[colnames(Counts), , drop = FALSE]
Genes <- Genes[rownames(Counts), , drop = FALSE]
```

This forces the sample and gene metadata to follow the same order as the main Counts matrix, ensuring all three datasets stay aligned. 

### Create the ExpressionSet Object

In this step, I combined the three datasets (Counts, SampleInfo, and Genes) into a single combined structure called an ExpressionSet, which is the standard Bioconductor format for keeping gene expression data and all its related information in one place. 

Before building it, I used as.matrix() on the Counts table to make sure the data were stored as numeric values. ExpressionSets can only hold numeric expression data, so this step ensures that everything is in the right format before being combined, especially since R can sometimes read numbers as text when importing files.

Unlike regular data frames, AnnotatedDataFrames keep data organised and consistent by automatically checking that row names and variable labels align across the dataset. They also store metadata (the information that describes what the data represent, such as sample conditions or gene annotations). This prevents mix-ups between samples or genes and ensures full compatibility with Bioconductor tools for reliable, reproducible analysis:

```{r}
## Create an ExpressionSet object that combines expression data with sample and gene metadata
eset <- ExpressionSet(
  ## Convert the Counts dataframe into a numeric matrix 
  ## ExpressionSets require numerical expression values for analysis
  assayData = as.matrix(Counts), 
  ## Wrap the sample information into an AnnotatedDataFrame
  ## This becomes the phenoData, describing the experimental conditions for each sample
  phenoData = AnnotatedDataFrame(SampleInfo), 
  ## Wrap the gene information (Genes) into an AnnotatedDataFrame
  ## This becomes the featureData, storing metadata for each gene (e.g.symbol, chromosome)
  featureData = AnnotatedDataFrame(Genes)
)
## Print the ExpressionSet object to confirm it was created successfully
## This displays a summary showing how many genes (features) and samples are included,
## along with the metadata variables stored in each section
eset ## Check the object
```
### Results Interpretation

The datasets were imported and aligned correctly, and successfully combined into an ExpressionSet containing 18,145 features and 76 samples. Checks confirmed that gene and sample identifiers matched across all files, and the final object was structured correctly for downstream analysis.

# Question 2: Print out the mean expression for each sample (3%)

The expression data were taken from the ExpressionSet using exprs() and the average expression for each sample was calculated with colMeans(). This gives a quick overview of how strongly each sample is expressed overall and helps to spot any that look unusually high or low, which could point to technical variation or possible outliers.
I used exprs() from the Biobase package because it’s the standard, reliable way to pull out the numeric data stored inside an ExpressionSet. The base R function colMeans() was chosen because it’s simple and efficient. It calculates averages across all samples in one step, making it easy to compare expression levels and check data consistency.

```{r}
## Extract expression data from the ExpressionSet
ExpressionData <- exprs(eset)
## Calculate mean expression value for each sample (column)
MeanExpression_PerSample <- colMeans(ExpressionData)
## Print the mean expression for each sample
MeanExpression_PerSample
```
### Results Interpretation

Overall, the mean expression levels look pretty consistent across most samples which appear around the 400-700 mark, suggesting the data is generally stable and reliable. However, a few samples such as 8, 15, and 75 are very low compared to the rest, which could mean there were technical issues or just natural variation that’s worth looking into more closely.

# Question 3: Find the gene with the highest median expression (5%)

To identify the gene with the highest overall expression, I first calculated the median expression value for each gene across all samples using the base R apply() function. The median was chosen instead of the mean because it provides a more robust summary that is less affected by extreme values or outliers. Next, I sorted these median values in descending order using sort(), which allowed me to quickly rank the genes by their expression levels. Finally, I extracted the top-ranked gene and its corresponding median value to identify the most consistently and abundantly expressed gene in the dataset. Both apply() and sort() are base R functions, making this method efficient and easily reproducible without relying on further packages.

```{r}
## Calculate the median expression value for each gene across all samples
## The '1' argument applies the median function across rows (genes), not columns (samples)
Medians_per_gene <- apply(ExpressionData, 1, median)
## Sort all genes by their median expression in descending order
## This ranks genes from the highest to lowest median expression
Medians_per_gene_sorted <- sort(Medians_per_gene, decreasing = TRUE)
## Extract and display the top-ranked gene with the highest median expression
## The first entry in the sorted vector corresponds to the top-expressed gene
print("Gene with the highest median expression:")
Top_gene <- Medians_per_gene_sorted[1]
Top_gene
```
 
### Results Interpretation
 
The gene RMRP had the highest median expression across all samples, with a value of about 315,451. This means it’s one of the most consistently and strongly expressed genes in the dataset. Such high and steady expression usually points to a gene with an essential housekeeping or structural role, rather than one that varies between conditions.
 
# Question 4: Find the gene with the lowest standard deviation (5%)

In this section, I calculated the standard deviation of expression for each gene across all samples using apply(exprs(eset), 1, sd). This measures how much a gene’s expression levels vary between samples, helping to identify those that are expressed most consistently. By sorting these values in ascending order, I was able to determine the gene with the lowest overall variation in expression. 

```{r}
## Calcaulte SD per gene
SD_per_gene <- apply(exprs(eset), 1, sd) 
## Sort in ascending order (lowest SD first)
SD_per_gene_ascending <- sort(SD_per_gene, decreasing = FALSE) 
## Print the gene with the lowest standard deviation
print("Gene with the lowest standard deviation:")
SD_per_gene_ascending[1]
## View the first few to check it is the lowest SD
print("View the first few genes (lowest SD first) to check")
options(digits=20) ## Prints 20 significant figures to ensure [1] was the lowest SD 
head(SD_per_gene_ascending) ## This confirms [1] is the lowest SD
```
### Results Interpretation

The results show that RBMY1D had the lowest variation in expression across all samples, meaning its expression stayed very consistent. This suggests it’s likely a stable, housekeeping-type gene that’s active in all samples and not strongly affected by experimental conditions. I had to extend the number of decimal places to 20 as there were originally top 2 genes listed with the lowest SD (RBMY1D and PPME1). By extending the decimals, I saw that RBMY1D had a smaller SD number when comparing the two, so RBMY1D is correctly answered as the gene with the lowest SD.

# Question 5: Determine how many samples were in each batch (2%)

To find out how many samples were processed in each batch, I used pData() from the Biobase package to extract the sample metadata from the ExpressionSet. This function provides access to the phenotypic information associated with each sample, including its batch assignment. I then applied the base R function table() to the Batch column to count how many samples were included in each group. This approach was used because it is simple and reproducible, allowing verification of batch sizes without needing further packages.

```{r}
## Extract the sample metadata (phenoData) from ExpressionSet  
## Contains information about what sample is in each batch
pData(eset)
## Count how many samples are in each batch
print("Number of samples in each batch:")
table(pData(eset)$Batch) ## pData(eset)$Batch extracts the batch column from the sample metadata. table() counts each time a batch name appears.
```
### Results Interpretation

There were 76 samples in total, spread across seven batches. Batch 3 and Batch 4 had the most samples, with 24 and 21 respectively, while the other batches were much smaller, with between 6 and 8 samples each. This uneven distribution is worth keeping in mind, as it could influence results if batch effects are present later in the analysis.

# Question 6: Plot a boxplot of the expression of the top 10 genes ranked by standard deviation (hint: base R or ggplot2 is acceptable) (10%)

### Rank the data in descending order of variability

In this section, I looked for the genes that varied the most across samples by calculating their standard deviation. Genes with higher variation are more likely to respond to different conditions or show meaningful biological differences. I used the base R function sort() to rank the genes from highest to lowest variability because it’s simple and doesn’t need extra packages. Then, I used names() to pull out the top 10 most variable genes, which makes it easy to focus on those that might be biologically interesting when plotting their expression patterns.

```{r}
## Rank genes by standard deviation descending order (highest variability first)
SD_per_gene_descending <- sort(SD_per_gene, decreasing = TRUE)
## Isolate top 10 genes
print("Top 10 genes with the highest variability:")
Top10Genes <- names(SD_per_gene_descending[1:10])
Top10Genes
```

The top 10 most variable genes were RMRP, SNORA63, RPPH1, SNORA8, STATH, and others. These genes showed the biggest differences in expression between samples, suggesting they may be affected by biological differences or experimental conditions. Genes with this kind of variability are often the most interesting to explore further, as they can reveal changes linked to specific pathways or cell behaviours.

### Convert wide format to long format for ggplot2 usage

In this step, I converted the expression data from a wide format (genes as rows and samples as columns) into a long format suitable for use with ggplot2. This structure is required for most tidyverse visualisations, as each row represents a single observation; in this case, a unique combination of a gene, sample, and its expression value. The pivot_longer() function was used to reshape the data, creating clear columns for Gene_ID, Sample_ID, and Expression_Value. Before reshaping, rownames_to_column() from the tibble package was applied to preserve gene identifiers as a separate column, ensuring no information was lost during the transformation. This tidy data format allows for more flexible and efficient plotting in ggplot2.

```{r}
library(tibble)
library(Biobase)
## Convert wide dataframe to long dataframe for using ggplot2
## Convert the ExpressionSet from wide format to long format for use with ggplot2
Counts_long <- exprs(eset) %>% 
  as.data.frame() %>%  ## Convert matrix ExpressionSet to dataframe to use pivot_longer and rownames_to_column functions which only work on dataframes
        rownames_to_column("Gene_ID") %>% ## Move rownames into a new column called 'Gene_ID'
        pivot_longer(cols = -"Gene_ID", ## Transform all other columns (sample IDs) except 'Gene_ID'
                     names_to = "Sample_ID", ## Create a new column 'Sample_ID' containing former sample numbers
                     values_to = "Expression_Value") ## Create a new column 'Expression_Value' containing the corresponding values
## View the resulting long-format dataframe
Counts_long
```
The data were successfully reshaped into long format, where each row now shows one gene, one sample, and its expression value. The final dataset contains over 1.3 million rows, meaning all gene–sample pairs were kept. This format is much easier to use for plotting and exploring expression patterns in ggplot2.

### Plot boxplot

To look at how the ten most variable genes differed in their expression, I used a mix of tidyverse and statistical packages to prepare the data, run the analysis, and create the plot. I used dplyr to filter and tidy the data so I could focus on the top ten genes and keep them in the right order on the x-axis. ggplot2 was used for plotting because it makes clean, flexible graphs, and ggpubr helped me easily add the ANOVA p-value straight onto the plot. I also used scales to make the axis labels look neater, and emmeans and multcompView to handle the post-hoc Tukey tests and create the compact letter groupings.

I first prepared the dataset by filtering the long-format expression data to only include the ten most variable genes. This kept the figure clear and focused on the genes that showed the biggest changes across samples. I then ran a one-way ANOVA to test whether the mean expression levels differed between those genes overall. ANOVA was the best choice here because it lets you compare several groups at once rather than doing multiple t-tests, which could increase the risk of false positives. The results showed a clear difference between genes, so I followed up with Tukey’s HSD test to find out exactly which pairs of genes were significantly different from each other. I chose Tukey’s test because it automatically adjusts for the fact that you’re making many comparisons, which helps prevent false positives. Other methods, like running lots of separate t-tests, can easily overstate significance because they don’t correct for that. Tukey’s test gives a more reliable picture of which differences are genuinely meaningful.

The pairwise p-values from Tukey’s test were turned into simple letter groups using multcompView. Genes that share the same letter aren’t significantly different, while genes with different letters are. I used letters instead of the usual brackets because they look cleaner and make the plot easier to read when comparing many genes at once. These letters were stored in a small dataframe so they could be added neatly to the ggplot later.

Finally, I plotted everything using ggplot2, adding boxplots to show the distribution of expression values for each gene and jittered points to display individual samples. I included the overall ANOVA p-value on the plot so the significance was clear at a glance. The axes and title were labelled to make the figure self-explanatory, and I used a minimal theme for a clean layout. I also added a little extra space at the top so the letters and p-value labels didn’t overlap with the boxes. Altogether, this approach made the analysis easier to interpret.

```{r}
## Load required packages
library(dplyr) ## data wrangling (filter, mutate, pipes)
library(ggpubr) ## stat_compare_means() to add ANOVA p-value on plot
library(ggplot2) ## plotting
library(scales) ## nicer axis label helpers (non-critical but helpful)
library(emmeans) ## estimated marginal means 
library(multcompView) ## converts Tukey p-values into compact letter display

## --------- Prepare the plotting dataset ---------------------------------------------
Top10_long <- Counts_long %>% ## Use existing long dataframe 
  filter(Gene_ID %in% Top10Genes) %>% ## Isolate the top 10 genes 
  mutate(
    Gene_ID = factor(Gene_ID, levels = Top10Genes)
    ) ## Make sure genes stay in this order on x-axis

## ------- Perform one-way ANOVA across 10 genes --------------------------------------
## ANOVA checks if there are differences in mean expression between 10 genes
anova_result <- aov(Expression_Value ~ Gene_ID, data = Top10_long) 
summary(anova_result) ## shows F-value and p-value (significance of the test)

## ------- One-way ANOVA Interpretation -----------------------------------------------
## Write out a short interpretation of the ANOVA test result:
print("A one-way ANOVA revealed a highly significant effect of gene identity on expression levels across the top 10 most variable genes (F(9, 750) = 1244.13, p < 2.2 × 10). This indicates that mean log-transformed expression differs substantially between genes, confirming that the selected genes show markedly different expression profiles across samples.")

## -------- Post-hoc test(Tukey HSD) --------------------------------------------------
## This tells us exacrtly which genes differ from each other
fit <- aov(Expression_Value ~ Gene_ID, data = Top10_long) ## Reuse the ANOVA model

## -------- Get pairwise comparisons from Tukey's test --------------------------------
tk <- TukeyHSD(fit, "Gene_ID")$Gene_ID 
pvals <- tk[,"p adj"] ## take the adjusted p-values for each gene pair
names(pvals) <- rownames(tk) ## label them so we know which comparisons they belong to 

## Turn those results into simple letter groups (eg. a, b, c)
## Genes that share the same letter are NOT significantly different
letters_df <- multcompView::multcompLetters(pvals, threshold = 0.05)$Letters

## Put those letters into a small dataframe so ggplot can use them later
cld_tbl <- data.frame(
  Gene_ID = names(letters_df),
  Letters = letters_df,
  row.names = NULL
)

## Set where to place the letters on the plot 
## We will put them slightly above the tallest box so they don't overlap with the data
y_letters <- max(Top10_long$Expression_Value, na.rm = TRUE) * 1.05

## --------- Draw the boxplot --------------------------------------------------------
ggplot(Top10_long, aes(Gene_ID, y = Expression_Value, fill = Gene_ID)) +
  geom_boxplot(width = 0.6, alpha = 0.7, outlier.shape = 16, outlier.size = 1.2) + ## boxplot shows spread
  geom_jitter(width = 0.15, alpha = 0.45, size = 0.8, colour = "black") + ## dots show individual samples
  
## Add the ANOVA p-value onto the plot 
  stat_compare_means(
    method = "anova",
    label = "p.format",
    label.y = y_letters * 1.03 ## Keep on data scale (not LogExpression)
  ) +
  
## Add the letter group labels from the Tukey test
  geom_text(
    data = cld_tbl,
    aes(x = Gene_ID, y = y_letters, label = Letters),
    inherit.aes = FALSE, size = 4
  ) + 
  
## Give the plot a title and clear axis labels
  labs(title = "Expression Distribution of Top 10 Most Variable Genes", 
       y = "Expression (raw count)", 
       x = "Gene") +
## Use a clean layout and make gene names readable
  theme_minimal(base_size = 12) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none") +
  
## Add a little extra space at the top so nothing is cut off
  expand_limits(y = y_letters * 1.08)
```
### Graph Interpretation
The boxplot above shows the distribution of raw expression values for the ten most variable genes in the dataset. Each box represents the middle 50 % of expression values for that gene, the horizontal line inside shows the median, and the dots show individual sample points.

A one-way ANOVA tested whether mean expression differed among the ten genes. The result was highly significant (F (9, 750) = 1244.13, p < 2.2 × 10^-16), meaning that the average expression levels are not the same across genes. The overall p-value is printed above the plot as "p = <2e-16" and is p < 0.05, suggesting expression counts between some genes are significantly different.

To find out which genes are different, a Tukey HSD post-hoc test was carried out. The small letters above each box show the Tukey groupings.

Genes that share at least one letter (for example a and b) are not significantly different in mean expression.

Genes with different letters differ significantly (p < 0.05).

According to the Tukey letter groupings, RMRP and SNORA63 (group a) showed the highest mean expression levels and were not significantly different from each other. RPPH1 (group b) had moderately lower expression but was still significantly higher than all genes in group c. The remaining genes — SNORA8, STATH, PGA4, PRH2, SNORA70, SNORA64, and SNORA24 — formed a distinct low-expression cluster.

The top-expressed genes, RMRP, SNORA63, and RPPH1, are all non-coding RNAs associated with rRNA processing, ribosome biogenesis, and RNA maturation, which likely explains their high abundance and variability across samples. In contrast, genes such as STATH (a salivary antimicrobial peptide), PGA4 and PRH2 (digestive or salivary enzymes), and the SNORA family members (involved in small nucleolar RNA modification) displayed much lower expression overall, consistent with more specialised or tissue-restricted activity.

### Assessment 3 40% 

## Write functions that can do the following:

# Question 1: Returns a data frame containing the mean, standard deviation and median of each column in a dataframe (e.g. mtcars, or the file Sales.tsv). (10%)

I created a function that summarises each numeric column in a dataset by calculating its mean, standard deviation, and median. The idea was to make a quick and reusable tool that gives a clear overview of any dataset’s main statistics in just one step. Before running the calculations, the function first checks which columns are numeric so it doesn’t try to summarise non-numeric data like sample names, IDs, or text labels. This helps prevent errors and keeps the output focused only on the variables that make sense to analyse numerically.

To perform the calculations, I used sapply() with na.rm = TRUE. I chose sapply() because it applies a function (like mean, sd, or median) to each column automatically and returns the results in a simple, clean table format. It’s also faster than using a loop, and it automatically simplifies the output into a vector or data frame — which means I don’t need to manually clean or reshape the results afterwards. The na.rm = TRUE part tells R to ignore any missing values when calculating, so the function doesn’t stop or produce errors if the dataset contains NAs. This makes it more robust and reliable for real-world data, which often includes some missing entries.

I decided to store the results in a new data frame because it presents the summary neatly, with each variable shown alongside its calculated mean, standard deviation, and median. I also added an option to rename the first column (for example, to “Month” or “Feature”) so the output is easy to interpret when applying the function to different datasets. This small detail helps the summary make sense in context, especially when the results are shared or printed.

I chose to keep the function based entirely on base R functions like sapply() and data.frame() instead of using tidyverse functions because it keeps the code simple and compatible with any R environment without needing to load extra packages. Finally, I rounded the output for readability and returned it as a tibble, so that when it’s displayed in the knitted HTML report, it looks tidy and easy to read.

```{r}
## Define a function to calculate summary statistics for each column in the mtcars dataframe
summary_mtcars <- function(df){
  ## Calculate the mean for each column
  ## 'sapply()' applies the mean function to every column in 'df'
  ## 'na.rm = TRUE' ensures missing values are ignored
  column_means <- sapply(df, mean, na.rm = TRUE)
  ## Calculate the standard deviation for each column
  ## This measures how spread out the values are around the mean
  column_sd <- sapply(df, sd, na.rm = TRUE)
  ## Calculate the median for each column
  ## The median is included as it’s more robust to outliers than the mean
  column_medians <- sapply(df, median, na.rm = TRUE)
  ## Combine all three statistics into a single summary dataframe
  ## Each row represents a variable (column from the original dataset)
  ## and each column represents one summary statistic
  new_mtcars <- data.frame(
    Means = column_means, 
    Standard_Deviation = column_sd, 
    Medians = column_medians
  ) 
  
  ## Return the completed summary dataframe
  ## This allows the user to quickly see the mean, SD, and median for all columns
  return(new_mtcars)
}
## Call the function using the built-in 'mtcars' dataset
## This will display the mean, SD, and median for all numeric columns in 'mtcars'
summary_mtcars(mtcars)
```
### Results Interpretation

This table gives an overview of each numeric column in the mtcars dataset. It shows the average (means), middle value (medians), and how much the numbers vary (standard deviations). For example, cars get about 20 miles per gallon on average, but there’s a fair bit of variation between models. Engine displacement (disp) has the biggest spread, which means engine sizes differ a lot across the cars. On the other hand, things like drat and vs don’t change much between models. This is a straight-forward way to see which car features are similar and which ones are more different across the dataset.

## For questions (2) and (3) below, use the following DNA sequence 
# exampleDNASequence <-c("TTGGTCATCCTAGACACGCGCCCTACCTGTCAAAATCTAAAATTCATCATACCCTGCGGACGGTGCTTCTGTGCCGAGGCGCAGGCCGATATGTTTCTAC")

# Question 2: PART 1 - Take a string containing a DNA sequence, convert this to a vector, which each entry being one base. (Hint: strsplit in conjuction with unlist). (15%)

In this step, I broke the DNA sequence down into its individual bases so that each nucleotide (A, T, G, or C) could be easily accessed and analysed later. I used the base R functions strsplit() and unlist(), which together provide a quick and efficient way to turn a single DNA string into a vector of individual characters. strsplit() separates the sequence into single elements, and unlist() flattens the result into a clean character vector. This makes it much easier to count, replace, or manipulate specific bases in later parts of the analysis.

```{r}
## sequence
exampleDNASequence <-c("TTGGTCATCCTAGACACGCGCCCTACCTGTCAAAATCTAAAATTCATCATACCCTGCGGACGGTGCTTCTGTGCCGAGGCGCAGGCCGATATGTTTCTAC")
## use strsplit to separate into individual items
## unlist to convert list into individual character vector
split_letters <- unlist(strsplit(exampleDNASequence, ""))
split_letters
```
The DNA sequence was successfully split into individual letters. Each base (A, T, G, or C) is now stored separately and the sequence is now easy to work with. For example, we can check, count, or change any base individually. The output confirms that the data has been correctly prepared for the next steps.

# PART 2 - Calculate the length of the sequences, the amount of each of the four nucleotides, A, T, G and C. Return these as a vector with each item named (i.e. length, A, T, G, C)

In this step, I wrote my own function called count_letters() to count how many times each DNA base (A, T, G, and C) appears in the sequence. I wanted to show that I understand how loops and if statements work in R by building the counting logic from scratch rather than relying on built-in shortcuts.

Inside the function, I first created four variables (A, C, G, and T) and set them all to zero. These act like counters that start at zero and increase as the function goes through the sequence. Then, I used a for loop to go through each position in the sequence one by one. The loop runs from 1:length(split_letters), which means it starts at the first base and ends at the last one, making sure no base is skipped.

Inside the loop, I used a variable called base to store the current letter in the sequence. This makes the code easier to read and saves me from having to type split_letters[i] every time. Then, I added a set of if and else if statements to check which base it was. If the current letter was “A”, then the A counter increased by 1. The same logic was used for the other three bases — if it was “C”, “G”, or “T”, the matching counter increased by 1. Each time the loop ran, one of the four counters went up by one, depending on which base was found.

After the loop finished checking every letter, I combined all the results into a named vector using the c() function. This vector contains five values: the total sequence length, and the counts of A, T, G, and C. I used names for each value (like length =, A =, etc.) so that it’s really clear what each number means. Finally, I used R’s table() function to double-check that my results were correct. table() automatically counts how many times each base appears, so I could compare that output with the numbers from my custom function. They matched perfectly, which confirmed that my loop worked properly.

I wrote the function this way because it’s easy to understand and shows how R processes data step by step. It also demonstrates that I can use basic programming tools (loops, variables, indexing, and conditionals) to solve problems without relying on pre-built functions. This approach is slower than using something like table(), but it’s a good way to learn and prove I understand how the counting logic actually works.
    
```{r}
## Define a function to count the number of each nucleotide in a DNA sequence
count_letters <- function(split_letters) {
  
  ## Initialise counters for each base (A, C, G, T)
  ## Start all counters at zero so we can increment them as we loop through the sequence
  A <- 0
  A <- 0
  C <- 0
  G <- 0
  T <- 0
  
  ## Use a for loop to go through every position in the DNA sequence
  ## The loop runs once for each element in 'split_letters'
  for (i in 1:length(split_letters)) {
    
    ## Extract the current nucleotide from the sequence
    base <- split_letters[i] ## Extract the actual base letter not the index 
    
    ## Check which base it is, and add 1 to the correct counter
    ## If the base is "A", increase the A counter by one
    if (base == "A") {
      A <- A + 1
    }
    ## If the base is "C", increase the C counter by one
    else if (base == "C") {
      C <- C + 1
    }
    ## If the base is "G", increase the G counter by one
    else if (base == "G") {
      G <- G + 1
    }
    ## If the base is "T", increase the T counter by one
    else if (base == "T") {
      T <- T + 1
    }
  }    
  
  ## Combine all results into a named vector
  ## This creates a clear summary showing the total sequence length and counts for each base
  result = c(
    length = length(split_letters), ## total number of bases in the sequence
    A = A, ## number of adenine bases
    T = T, ## number of thymine bases
    G = G, ## number of guanine bases
    C = C ## number of cytosine bases
  )
    ## Return the result so it can be displayed or stored
    return(result)
}
## Run the function on the DNA sequence that was previously split into individual letters
count_letters(split_letters)

## Cross-check the counts using base R functions for verification
## 'table()' automatically counts how many times each base appears
## 'length()' confirms the total number of bases in the sequence
print("Double check with alternative method:")
table(split_letters)
length(split_letters)

```
### Results Interpretation

The output shows that the DNA sequence is 100 bases long, meaning it contains a total of 100 individual nucleotides. Within that sequence, there are 22 adenines (A), 26 thymines (T), 22 guanines (G), and 30 cytosines (C).

These numbers tell us the overall composition of the DNA strand. In other words, how many of each base type it contains. The sequence appears to be fairly balanced, but there are slightly more C and T bases compared to A and G. This is typical in many DNA segments, where base composition can vary depending on the organism or genomic region.

The second part of the output, ie. the check using table(split_letters), shows the exact same values as the custom function, confirming that the counting logic worked correctly. Both methods agree, which means the function was written properly and accurately calculated the number of each base in the sequence.

Overall, this output confirms that the function successfully counted all nucleotides, produced the correct sequence length, and matched the built-in verification method, showing that the code is working exactly as intended.

# Question 3: Write a function that takes a DNA sequence and substitutes one nucleotide specified by an argument (default value “A”) for another nucleotide specified by an argument (default value “T”). This function should return a named list with both the original and the new string of nucleotides. (15%)

In this task, I wrote a simple function that replaces one DNA base with another across an entire sequence. The example DNA sequence was provided for me, and I used it to test how well the function worked. I created the function build_sequence() with three inputs: one for the DNA sequence itself, one for the base I want to replace (old), and one for the new base that should take its place (new). I gave the old and new arguments default values of "A" and "T" so that the function automatically replaces A with T if no other inputs are given. This makes it quick to run but also easy to customise. For example, you can change the arguments to swap G for C or T for A without editing the code.

Inside the function, I used gsub() to perform the replacement. I chose gsub() because it carries out a global substitution, meaning it replaces every instance of the chosen base in the sequence, not just the first one. This is important in DNA because a base like “A” can appear many times along the strand, and we want every one of them to change. The new, modified version of the sequence is stored in a variable called new_sequence so that both the original and changed versions can be returned together. Finally, I used return() to output a small named list that contains both sequences, one labelled “original” and the other “modified.” This makes it really easy to compare them side by side and confirm that the function worked correctly.

When I ran the function using the given DNA sequence, every “A” was successfully replaced with a “T,” showing that gsub() correctly performed a full substitution across the entire string. This method is clean, efficient, and flexible, allowing any nucleotide swap to be done in just one line of code.
```{r}
## Given DNA sequence (provided in the question)
exampleDNASequence <-c("TTGGTCATCCTAGACACGCGCCCTACCTGTCAAAATCTAAAATTCATCATACCCTGCGGACGGTGCTTCTGTGCCGAGGCGCAGGCCGATATGTTTCTAC")

## Function that substitutes one nucleotide (default "A") for another (default "T")
build_sequence <- function(dna_sequence = exampleDNASequence, old = "A", new = "T") { 
  
  ## Use gsub() to perform a global substitution of all 'old' nucleotides with 'new'
  ## gsub() replaces every occurrence of the specified base across the whole sequence,
  ## unlike sub(), which would only replace the first instance
  new_sequence <- gsub(old, new, dna_sequence) 
  
  ## Return both the original and modified DNA sequences as a named list
  ## Returning both helps confirm that the substitution worked correctly
  return(list(
    original = dna_sequence,
    modified = new_sequence
  ))
}
## Example run (calls the function using the provided DNA sequence)
## Since default values are used, it replaces all "A" nucleotides with "T"
build_sequence(exampleDNASequence)

```
### Results Interpretation

The function worked as it replaced every “A” in the DNA sequence with “T”, as intended. In the output, the "original" part shows the DNA strand exactly as it was given, and the "modified" part shows the new version after the change. When comparing the two, you can clearly see that every “A” base in the original has been swapped for a “T”, while all the other bases stayed the same.

This confirms that the gsub() function successfully made the change across the entire sequence, not just at the first occurrence. Having both the original and modified sequences printed side by side also makes it easy to double-check that the function worked properly. Overall, the result shows that the function is reliable, simple, and effective for editing DNA sequences in R.

